{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "answer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a\n",
    "import requests\n",
    "import json\n",
    "import psycopg2\n",
    "\n",
    "\n",
    "\n",
    "api_url = 'https://api.example.com/data'\n",
    "\n",
    "\n",
    "conn = psycopg2.connect(database=\"your_database\", user=\"your_user\", password=\"your_password\", host=\"your_host\", port=\"your_port\")\n",
    "cur = conn.cursor()\n",
    "\n",
    "\n",
    "response = requests.get(api_url)\n",
    "\n",
    "\n",
    "data = response.json()\n",
    "\n",
    "for record in data:\n",
    "    cur.execute(\"INSERT INTO your_table (column1, column2) VALUES (%s, %s)\", (record['value1'], record['value2']))\n",
    "\n",
    "\n",
    "conn.commit()\n",
    "cur.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b\n",
    "from kafka import KafkaConsumer\n",
    "import json\n",
    "\n",
    "# Example: Consuming sensor data from a Kafka topic and processing it\n",
    "\n",
    "# Kafka broker configuration\n",
    "bootstrap_servers = 'your_kafka_broker:9092'\n",
    "topic = 'your_topic'\n",
    "\n",
    "# Create a Kafka consumer\n",
    "consumer = KafkaConsumer(topic, bootstrap_servers=bootstrap_servers)\n",
    "\n",
    "# Process incoming messages\n",
    "for message in consumer:\n",
    "    # Decode the message value\n",
    "    data = json.loads(message.value)\n",
    "\n",
    "    # Perform data processing or analysis\n",
    "    # Example: Calculate average temperature from sensor data\n",
    "    temperature_values = data['temperature']\n",
    "    avg_temperature = sum(temperature_values) / len(temperature_values)\n",
    "\n",
    "    # Do something with the calculated average temperature\n",
    "    print(f\"Average temperature: {avg_temperature}\")\n",
    "\n",
    "    # Additional data processing logic...\n",
    "\n",
    "# Close the Kafka consumer\n",
    "consumer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c-\n",
    "import csv\n",
    "import json\n",
    "\n",
    "# Example: Reading data from CSV and JSON files, performing validation, and data cleansing\n",
    "\n",
    "# CSV file processing\n",
    "with open('data.csv', 'r') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "    \n",
    "    for row in csv_reader:\n",
    "        # Perform validation\n",
    "        if row['column1'] == '':\n",
    "            continue  # Skip rows with missing values\n",
    "        \n",
    "        # Perform data cleansing\n",
    "        row['column2'] = row['column2'].strip()\n",
    "        \n",
    "        # Process the data further or store it in a database\n",
    "        \n",
    "# JSON file processing\n",
    "with open('data.json', 'r') as json_file:\n",
    "    data = json.load(json_file)\n",
    "    \n",
    "    for record in data:\n",
    "        # Perform validation\n",
    "        if record.get('column1') is None:\n",
    "            continue  # Skip records with missing values\n",
    "        \n",
    "        # Perform data cleansing\n",
    "        record['column2'] = record['column2'].strip()\n",
    "        \n",
    "        # Process the data further or store it in a database\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "answer2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a-\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('customer_churn.csv')\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = data.drop('Churn', axis=1)\n",
    "y = data['Churn']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Perform feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train the model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-score: {f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('dataset.csv')\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = data.drop('target_variable', axis=1)\n",
    "y = data['target_variable']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('numeric', StandardScaler(), ['numeric_feature_1', 'numeric_feature_2']),\n",
    "        ('categorical', OneHotEncoder(), ['categorical_feature'])\n",
    "    ])\n",
    "\n",
    "# Define the model training pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('dim_reduction', PCA(n_components=10)),\n",
    "    ('classifier', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c-\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define the base model with pre-trained weights (MobileNetV2)\n",
    "base_model = MobileNetV2(input_shape=(224, 224, 3), include_top=False, weights='imagenet')\n",
    "\n",
    "# Freeze the base model layers\n",
    "base_model.trainable = False\n",
    "\n",
    "# Add additional layers for classification\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "# Create the final model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Data augmentation\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Load and preprocess the training and testing images\n",
    "train_generator = train_datagen.flow_from_directory('train_directory', target_size=(224, 224), batch_size=batch_size, class_mode='categorical')\n",
    "test_generator = test_datagen.flow_from_directory('test_directory', target_size=(224, 224), batch_size=batch_size, class_mode='categorical')\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_generator, steps_per_epoch=len(train_generator), epochs=epochs)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(test_generator, steps=len(test_generator))\n",
    "\n",
    "print(f\"Loss: {loss}\")\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "answer 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a-\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('housing_data.csv')\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = data.drop('Price', axis=1)\n",
    "y = data['Price']\n",
    "\n",
    "# Create the regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Convert negative mean squared error to positive root mean squared error\n",
    "rmse_scores = (-cv_scores)**0.5\n",
    "\n",
    "# Print the average RMSE score and its standard deviation\n",
    "print(f\"Average RMSE: {rmse_scores.mean()}\")\n",
    "print(f\"RMSE Standard Deviation: {rmse_scores.std()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b-\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('binary_classification_data.csv')\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = data.drop('target_variable', axis=1)\n",
    "y = data['target_variable']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create the logistic regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model using different metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-score: {f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c-\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load the imbalanced dataset\n",
    "data = pd.read_csv('imbalanced_data.csv')\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = data.drop('target_variable', axis=1)\n",
    "y = data['target_variable']\n",
    "\n",
    "# Create the model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Create a StratifiedKFold object for stratified sampling\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
    "\n",
    "# Perform cross-validation using stratified sampling\n",
    "for train_index, test_index in stratified_kfold.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    # Train and evaluate the model\n",
    "    model.fit(X_train, y_train)\n",
    "    accuracy = model.score(X_test, y_test)\n",
    "    print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "answer 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a-\n",
    "from flask import Flask, request, jsonify\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "# Load the trained model\n",
    "model = joblib.load('recommendation_model.joblib')\n",
    "\n",
    "# Create a Flask app\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Define the API endpoint for real-time recommendations\n",
    "@app.route('/recommend', methods=['POST'])\n",
    "def recommend():\n",
    "    user_data = request.json  # User interaction data in JSON format\n",
    "\n",
    "    # Preprocess the user data\n",
    "    user_df = pd.DataFrame(user_data)\n",
    "    \n",
    "    # Make recommendations using the trained model\n",
    "    recommendations = model.predict(user_df)\n",
    "    \n",
    "    return jsonify(recommendations.tolist())\n",
    "\n",
    "# Run the Flask app\n",
    "if __name__ == '__main__':\n",
    "    app.run()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "b-# AWS CodePipeline configuration (pipeline.yml)\n",
    "---\n",
    "Resources:\n",
    "  - Pipeline:\n",
    "      Name: ModelDeploymentPipeline\n",
    "      Stages:\n",
    "        - Name: Source\n",
    "          Actions:\n",
    "            - Name: SourceAction\n",
    "              ActionTypeId:\n",
    "                Category: Source\n",
    "                Owner: AWS\n",
    "                Provider: CodeCommit\n",
    "                Version: 1\n",
    "              Configuration:\n",
    "                RepositoryName: your_repository\n",
    "                BranchName: your_branch\n",
    "              OutputArtifacts:\n",
    "                - Name: source_code\n",
    "          ...\n",
    "        - Name: Build\n",
    "          Actions:\n",
    "            - Name: BuildAction\n",
    "              ActionTypeId:\n",
    "                Category: Build\n",
    "                Owner: AWS\n",
    "                Provider: CodeBuild\n",
    "                Version: 1\n",
    "              Configuration:\n",
    "                ProjectName: your_codebuild_project\n",
    "              InputArtifacts:\n",
    "                - Name: source_code\n",
    "              OutputArtifacts:\n",
    "                - Name: built_artifacts\n",
    "          ...\n",
    "        - Name: Deploy\n",
    "          Actions:\n",
    "            - Name: DeployAction\n",
    "              ActionTypeId:\n",
    "                Category: Deploy\n",
    "                Owner: AWS\n",
    "                Provider: ElasticBeanstalk\n",
    "                Version: 1\n",
    "              Configuration:\n",
    "                ApplicationName: your_eb_application\n",
    "                EnvironmentName: your_eb_environment\n",
    "              InputArtifacts:\n",
    "                - Name: built_artifacts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(filename='model_logs.log', level=logging.INFO)\n",
    "\n",
    "# Function to log errors\n",
    "def log_error(error_message):\n",
    "    logging.error(error_message)\n",
    "\n",
    "# Function to monitor model performance\n",
    "def monitor_performance():\n",
    "    # Monitor performance metrics and trigger alerts if thresholds are breached\n",
    "    # Example: Check response time, error rates, and throughput\n",
    "\n",
    "    if response_time > threshold:\n",
    "        log_error(f\"High response time: {response_time}\")\n",
    "    \n",
    "    if error_rate > threshold:\n",
    "        log_error(f\"High error rate: {error_rate}\")\n",
    "\n",
    "# Example usage of the monitoring functions\n",
    "try:\n",
    "    # Perform model prediction or recommendation\n",
    "    prediction = model.predict(data)\n",
    "\n",
    "    # Monitor performance\n",
    "    monitor_performance()\n",
    "    \n",
    "except Exception as e:\n",
    "    # Log any exceptions or errors\n",
    "    log_error(str(e))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
